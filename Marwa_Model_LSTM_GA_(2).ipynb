{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Marwa_Model_LSTM_GA (2).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarwaZAH/dissertatio-file/blob/main/Marwa_Model_LSTM_GA_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqLfv2HRqFHB"
      },
      "source": [
        "! pip install scikit-plot mlxtend\n",
        "! pip install -U scikit-learn\n",
        "#!python -c \"import sklearn; sklearn.show_versions()\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ksPHKiv_E2C"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jO0Hs5nkEE7"
      },
      "source": [
        "import pandas as pd\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "#from keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import Model\n",
        "from keras.layers import Lambda, Input, Dropout, Flatten, LSTM, Concatenate, Bidirectional, Conv1D, MaxPooling1D\n",
        "from keras import backend as K\n",
        "from keras.callbacks import TensorBoard\n",
        "from time import time\n",
        "from keras import optimizers\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from time import time\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.svm import SVR\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plot\n",
        "import seaborn\n",
        "from pandas import datetime\n",
        "import math, time\n",
        "import itertools\n",
        "import datetime\n",
        "from operator import itemgetter\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from math import sqrt\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers.recurrent import LSTM\n",
        "import sys\n",
        "from sklearn import linear_model\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t0jfNVZ_ICO"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guEKM0lkg04r"
      },
      "source": [
        "def return_shape(X_trainz,window):\n",
        "  size=X_trainz.shape[0]#* X_trainz.shape[1]\n",
        "  wind=window\n",
        "  shape0=0\n",
        "  for i in range(2,window):\n",
        "    if size%i==0:\n",
        "      #print(i,278343/i)\n",
        "      x=size/i\n",
        "      for f in range(2,window):\n",
        "        if x%f==0 and x*f==size:\n",
        "          shape0=x\n",
        "          wind=f\n",
        "          #print(x,f, x/f,'\\n-------- \\n')\n",
        "  return int(shape0),wind\n",
        "\n",
        "def return_shape2(X_trainz,window):\n",
        "  size=X_trainz.shape[0]#* X_trainz.shape[1]\n",
        "  wind=window\n",
        "  shape0=0\n",
        "  for i in range(2,window):\n",
        "    if size%i==0:\n",
        "      #print(i,278343/i)\n",
        "      x=size/i\n",
        "      for f in range(2,window):\n",
        "        if x%f==0:\n",
        "          shape0=size/f\n",
        "          wind=f\n",
        "          #print(x,f, x/f,'\\n-------- \\n')\n",
        "  return int(shape0),wind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdOoFkvA8tBE"
      },
      "source": [
        "#from sklearn.cross_validation import train_test_split\n",
        "#x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.4,random_state=4)\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "df  = pd.read_csv('/content/ma_23_ratios.csv')[:-3]\n",
        "# Training2 = pd.read_csv('SmartFall Training.csv')\n",
        "# df = pd.concat([Training2, Testing2], ignore_index=True)\n",
        "#df.describe()\n",
        "\n",
        "shape0,w1=return_shape2(df,25)\n",
        "#hape0,w1=return_shape(Training,25)\n",
        "print(df.shape,shape0,w1,shape0*w1,shape0*w1*3)\n",
        "#Training[['ms_accelerometer_x']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RudFE7Ma-uE"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRyCBqdibUUp"
      },
      "source": [
        "col=df.columns\n",
        "\n",
        "df=df[col[1:]]#.loc[:-3]\n",
        "df[-3:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_yzGfVu4gcc"
      },
      "source": [
        "df.isna().sum()\n",
        "#df.mean(skipna=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtDiphRF32hV"
      },
      "source": [
        "column_means = df.mean()\n",
        "#df = df.fillna(column_means)\n",
        "\n",
        "# df = df.replace('#DIV/0!', np.NaN)\n",
        "for i in df.columns[1:]:\n",
        "  # print(df[i].loc[df['R2']=='#DIV/0!'])\n",
        "  df[i] = df[i].replace('#DIV/0!', np.NaN)\n",
        "  df[i] = df[i].replace(',', '', regex=True)\n",
        "  df[i] = df[i].astype(float)\n",
        "  df[i].fillna(value=df[i].mean(), inplace=True)\n",
        "  print(i, df[i].isna().sum(), len(df.loc[df[i] == '#DIV/0!']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KjDPiCegHv6"
      },
      "source": [
        "# df.count()\n",
        "# df.isna().sum()\n",
        "# # df = df.loc[df['R2'] != '#DIV/0!']\n",
        "# # df.drop(df.loc[df['R2']=='#DIV/0!'].index, inplace=True)\n",
        "# column_means = df.mean()\n",
        "# #df = df.fillna(column_means)\n",
        "\n",
        "# df = df.replace('#DIV/0!', column_means)\n",
        "# df[-3:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTsNzAjrdXCT"
      },
      "source": [
        "df = df.replace(',', '', regex=True)\n",
        "df = df[1:].astype(float)\n",
        "print (df.dtypes)\n",
        "# df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9kv_jd6bc0_"
      },
      "source": [
        "# Old idea\n",
        "Train and test split from files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfY_UC72kLbN"
      },
      "source": [
        "# col=Training2.columns\n",
        "# total=int(Training2.shape[0]/w1)*w1\n",
        "# total1=int(Testing2.shape[0]/w1)*w1\n",
        "# Training=Training2.iloc[:total,:]\n",
        "# Testing=Testing2.iloc[:total1,:]\n",
        "\n",
        "# # split into train and test\n",
        "# lab_enc = preprocessing.LabelEncoder()\n",
        "# trainy = lab_enc.fit_transform(Training[col[-1]])\n",
        "# testy  = lab_enc.fit_transform(Testing[col[-1]])\n",
        "# #--------------------------------------------------------------------\n",
        "# n_train = 100\n",
        "# #y = to_categorical(y)\n",
        "# trainX, testX = Training[col[:-1]], Testing[col[:-1]]\n",
        "# #trainy, testy = to_categorical(Training[col[-1]]), to_categorical(Testing[col[-1]])\n",
        "\n",
        "# sc = StandardScaler()\n",
        "# trainX = sc.fit_transform(trainX)\n",
        "# testX  = sc.fit_transform(testX)\n",
        "\n",
        "# print(trainX.shape, testX.shape, trainy.shape ,testy.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFsjQfzfblGQ"
      },
      "source": [
        "# New idea \n",
        "- normal split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj4rYblTbp0H"
      },
      "source": [
        "col=df.columns\n",
        "X, y=df[col[1: ]], df[col[0]]\n",
        "X = preprocessing.minmax_scale(X)\n",
        "# split into train and test\n",
        "lab_enc = preprocessing.LabelEncoder()\n",
        "y = lab_enc.fit_transform(y)\n",
        "\n",
        "# split into train/test sets\n",
        "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "print(trainX.shape, testX.shape, trainy.shape ,testy.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02QJoyDkpPb-"
      },
      "source": [
        "trainy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eViFitqV_K4K"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr2EhDdwkxSI"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_model2(layers):\n",
        "        d = 0.2\n",
        "        #input_shape = X_train.shape\n",
        "        model = Sequential()\n",
        "        #Input_layer\n",
        "        model.add(LSTM(3, input_shape=(layers[0], layers[1]), return_sequences=True))\n",
        "        #model.add(Dropout(d))\n",
        "        #LSTM_layer\n",
        "        model.add(LSTM(30, input_shape=(layers[0], layers[1]), return_sequences=False))\n",
        "        #model.add(Dropout(d))\n",
        "        #model.add(Flatten())\n",
        "        model.add(Dense(30, activation='relu'))        \n",
        "        #model.add(Flatten())\n",
        "        #Output_layer\n",
        "        model.add(Dense(2, activation='sigmoid' ))\n",
        "        model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "        tf.keras.utils.plot_model(  model, to_file=\"model1.png\", show_shapes=True,\n",
        "    show_dtype=True,  show_layer_names=True, rankdir=\"TB\", expand_nested=False,  dpi=300,  layer_range=None,)\n",
        "        tf.keras.utils.model_to_dot(  model, show_shapes=True,  show_dtype=True,  show_layer_names=True, rankdir=\"TB\", expand_nested=False,  dpi=300,  layer_range=None,)\n",
        "        return model\n",
        "\n",
        "def build_model3(layers,Nueron,N_layer):\n",
        "        d = 0.2\n",
        "        #input_shape = X_train.shape\n",
        "        model = Sequential()\n",
        "\n",
        "        # The Input Layer :\n",
        "        model.add(LSTM(3, input_shape=(layers[0], layers[1]), return_sequences=True))\n",
        "        #model.add(Dropout(d))\n",
        "        # The Hidden Layers :\n",
        "        for i in range(N_layer-1):\n",
        "          model.add(LSTM(Nueron, input_shape=(layers[0], layers[1]), return_sequences=True))\n",
        "          #model.add(Dropout(d))\n",
        "        #model.add(Flatten())\n",
        "        model.add(LSTM(Nueron, input_shape=(layers[0], layers[1]), return_sequences=False))\n",
        "        model.add(Dense(Nueron, activation='relu'))        \n",
        "        #model.add(Flatten())\n",
        "\n",
        "        # The Output Layer :\n",
        "        model.add(Dense(2, activation='sigmoid'))\n",
        "        # Compile the network :\n",
        "        model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "        tf.keras.utils.plot_model(  model, to_file=\"model22.png\", show_shapes=True,\n",
        "    show_dtype=True,  show_layer_names=True, rankdir=\"TB\", expand_nested=False,  dpi=300,  layer_range=None,)\n",
        "        tf.keras.utils.model_to_dot(  model,  show_shapes=True,\n",
        "    show_dtype=True,  show_layer_names=True, rankdir=\"TB\", expand_nested=False,  dpi=300,  layer_range=None,)\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAddumN8_Ngs"
      },
      "source": [
        "#Setup the plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuowsuKO_RaY"
      },
      "source": [
        "get_ipython().magic('matplotlib inline')\n",
        "style = seaborn.axes_style(\"whitegrid\")\n",
        "style[\"axes.grid\"] = False\n",
        "seaborn.set_style(\"whitegrid\", style)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50T-a1V9_c9D"
      },
      "source": [
        "# Evaluate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QeCpF1D-2oU"
      },
      "source": [
        "from sklearn.metrics import auc,confusion_matrix,precision_recall_curve,precision_score,recall_score,roc_curve \n",
        "from sklearn.metrics import accuracy_score,f1_score,fbeta_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "#from sklearn.metrics import plot_precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import scikitplot as skplt\n",
        "\n",
        "def F_b(beta,recal,precision):\n",
        "  return (1+beta**2)*((recal*precision)/(recal+(beta**2)*precision))\n",
        "\n",
        "def plot_precision_recall_curve(model,testy2,yhat):\n",
        "  # calculate precision-recall curve\n",
        "  precision, recall, thresholds = precision_recall_curve(np.argmax(testy2, axis=1), np.argmax(yhat, axis=1))\n",
        "  # calculate F1 score\n",
        "  f1 = f1_score(np.argmax(testy2, axis=1), np.argmax(yhat, axis=1))\n",
        "  probas = model.predict(X_test2, batch_size=64)\n",
        "  skplt.metrics.plot_precision_recall_curve(y_test, probas)\n",
        "  plt.show()\n",
        "  #disp = plot_precision_recall_curve(classifier, X_test, y_test)\n",
        "  #disp.ax_.set_title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "def plot_history(history,name_f):\n",
        "  plt.plot(history.history['accuracy'], label='training accuracy')\n",
        "  plt.plot(history.history['val_accuracy'], label='testing accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.legend()\n",
        "  \n",
        "  file_name=name_f+\"_Accuracy.pdf\"\n",
        "  #dest1 = os.path.join(path, file_name)\n",
        "  plt.savefig(file_name, bbox_inches=\"tight\")\n",
        "  plt.show()\n",
        "\n",
        "def plot_history2(history,name_f):\n",
        "  plt.plot(history.history['loss'], label='training loss')\n",
        "  plt.plot(history.history['val_loss'], label='testing loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('loss')\n",
        "  plt.legend()\n",
        "  \n",
        "  file_name=name_f+\"_Loss.pdf\"\n",
        "  #dest1 = os.path.join(path, file_name)\n",
        "  plt.savefig(file_name, bbox_inches=\"tight\")\n",
        "  plt.show()\n",
        "\n",
        "def plot_auc(model, testy2, yhat, name_f):\n",
        "  # Binarize the output\n",
        "  y = label_binarize(testy2, classes=[0, 1])\n",
        "  n_classes = y.shape[1]\n",
        "  y_test=testy2\n",
        "  y_score=yhat\n",
        "  # Compute ROC curve and ROC area for each class\n",
        "  fpr = dict()\n",
        "  tpr = dict()\n",
        "  roc_auc = dict()\n",
        "  for i in range(n_classes):\n",
        "      fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
        "      roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "  # Compute micro-average ROC curve and ROC area\n",
        "  fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
        "  roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "  plt.figure()\n",
        "  lw = 2\n",
        "  plt.plot(fpr[1], tpr[1], color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[1])\n",
        "  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Receiver operating characteristic example')\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  file_name=name_f+\"_AUC_ROC.pdf\"\n",
        "  #dest1 = os.path.join(path, file_name)\n",
        "  plt.savefig(file_name, bbox_inches=\"tight\")\n",
        "  plt.show()\n",
        "\n",
        "def Evaluate_model(model,history,name_file,algo,time,score_table,X_test,y_test):\n",
        "  #X_train, y_train, X_test, y_test\n",
        "  testy2 = to_categorical(y_test)\n",
        "  loss, acc = model.evaluate(X_test, testy2, verbose=0)\n",
        "  print('Model Accuracy: %.3f' % acc)\n",
        "\n",
        "  # evaluate model on test set np.argmax(y_pred, axis=1)\n",
        "  yhat = model.predict(X_test)\n",
        "  acc = accuracy_score(np.argmax(testy2, axis=1), np.argmax(yhat, axis=1))\n",
        "   \n",
        "  precision, recall, f1, support = precision_recall_fscore_support(np.argmax(testy2, axis=1), np.argmax(yhat, axis=1))\n",
        "  precision=np.average(precision, weights=support)\n",
        "  recall=np.average(recall, weights=support)\n",
        "  f1=np.average(f1, weights=support)\n",
        "  fb = fbeta_score(np.argmax(testy2, axis=1), np.argmax(yhat, axis=1), average='micro', beta=3)\n",
        "\n",
        "  score_table.loc[algo,:] = time, acc, precision,recall,f1, F_b(3,recall,precision),fb\n",
        "  CM = confusion_matrix(np.argmax(testy2, axis=1), np.argmax(yhat, axis=1))\n",
        "  fig, ax = plot_confusion_matrix(conf_mat=CM ,  figsize=(5, 5))\n",
        "  plt.show()\n",
        "\n",
        "  plot_precision_recall_curve(model,testy2,yhat)\n",
        "  plot_auc(model,testy2,yhat,name_file)\n",
        "  # print(history.history.keys())\n",
        "  plot_history(history,name_file)\n",
        "  plot_history2(history,name_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6ejSuCjRcJv"
      },
      "source": [
        "#Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny_AzuWPRiqQ"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# df1  = pd.read_csv('/content/ma_23_ratios.csv')\n",
        "# df1 = df1.replace(',', '', regex=True)\n",
        "# df = df1[1:].astype(float)\n",
        "\n",
        "# min_max_scaler = preprocessing.MinMaxScaler()\n",
        "# col=df.columns\n",
        "# X, y=df[col[2: ]], df[col[1]]\n",
        "# x_scaled = min_max_scaler.fit_transform(X)\n",
        "# # split into train and test\n",
        "# lab_enc = preprocessing.LabelEncoder()\n",
        "# y = lab_enc.fit_transform(y)\n",
        "# kddcup = pd.DataFrame(x_scaled)\n",
        "# kddcup.columns=X.columns\n",
        "\n",
        "col=df.columns\n",
        "X, y=df[col[1: ]], df[col[0]]\n",
        "x_scaled = preprocessing.minmax_scale(X)\n",
        "X = pd.DataFrame(x_scaled)\n",
        "X.columns=df[col[1: ]].columns\n",
        "# split into train and test\n",
        "lab_enc = preprocessing.LabelEncoder()\n",
        "y = lab_enc.fit_transform(y)\n",
        "\n",
        "# split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "print(trainX.shape, testX.shape, trainy.shape ,testy.shape)\n",
        "# split into train/test sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(kddcup, y, test_size=0.2, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQNG9Q9UWBe-"
      },
      "source": [
        "y_train[1:9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkbCyjbjTfhY"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import *\n",
        "rcParams['figure.figsize'] = 15,8\n",
        "\n",
        "feature_names = [f'R{i}' for i in range(X.shape[1])]\n",
        "forest = RandomForestClassifier(random_state=0)\n",
        "forest.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxcNGEa4TmzJ"
      },
      "source": [
        "start_time = time.time()\n",
        "importances = forest.feature_importances_\n",
        "std = np.std([\n",
        "    tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(f\"Elapsed time to compute the importances: \" f\"{elapsed_time:.3f} seconds\")\n",
        "\n",
        "\n",
        "forest_importances = pd.Series(importances, index=feature_names)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.plot.bar(yerr=std, ax=ax)\n",
        "ax.set_title(\"Feature importances\")\n",
        "ax.set_ylabel(\"Mean decrease in impurity\")\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLgk2emCUEmh"
      },
      "source": [
        "new_features=forest_importances.sort_values(ascending=False).head(10)\n",
        "new_features.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkA8oBnDUfV4"
      },
      "source": [
        "X[new_features.index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjobxdOjUx8D"
      },
      "source": [
        "X_new=X[new_features.index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc257Umn_75I"
      },
      "source": [
        "# Data split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iI1mGECptiu"
      },
      "source": [
        "from numpy import array\n",
        "# (NumberOfExamples, TimeSteps, FeaturesPerStep)\n",
        "# X_train = train_inputs.reshape((split,3,2))\n",
        "# X_test = X_test.reshape((test_inputs.shape[0], 3, 2))\n",
        "# you have 3 steps and 2 features\n",
        "#TimeSteps\n",
        "window = 1\n",
        "X_train,  X_test, y_train, y_test  = train_test_split(X_new, y, test_size=0.2, random_state=2)\n",
        "\n",
        "X_train2 = np.reshape(array(X_train), (int(X_train.shape[0]/window), window, X_train.shape[1]))\n",
        "X_test2  = np.reshape(array(X_test), (int(X_test.shape[0]/window), window,  X_test.shape[1] ))\n",
        "\n",
        "y_train=y_train[:int(X_train.shape[0]/window)]\n",
        "y_test=y_test[:int(X_test.shape[0]/window)]\n",
        "# X_train2 = array(X_train).reshape( X_train.shape[0],1, X_train.shape[1])\n",
        "# X_test2 = array(X_test).reshape(X_test.shape[0],1, X_test.shape[1])\n",
        "X_train2.shape,y_train.shape, X_test2.shape,y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkbHoNq1Unkg"
      },
      "source": [
        "# Table to save final results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkcH3YfoUn5k"
      },
      "source": [
        "# initialize a score table to log the performance of various algorithms\n",
        "index = ['LSTM_'+str(n) for n in [20,30,50,80]]\n",
        "score_table_2 = pd.DataFrame(index = index, \n",
        "                             columns= ['Time','Accuracy','precision','recall','F1','F_b','Fb_metric'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFrbuUpdACYX"
      },
      "source": [
        "# Models fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynNHwGZ20xeG"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "import time\n",
        "\n",
        "  \n",
        "def fit_model(window1,Nueron1,N_layer1,X_train_f,y_train_f,X_test_f,y_test_f,Name1,path_model,score_table):\n",
        "  layers=[window1,X_train_f.shape[2]]\n",
        "  Nueron=Nueron1\n",
        "  N_layer=N_layer1\n",
        "  start_time = time.clock()\n",
        "\n",
        "  model = build_model3(layers,Nueron,N_layer)\n",
        "  y_train2 = to_categorical(y_train_f)\n",
        "  print(\"X_train_f: \",X_train_f.shape)\n",
        "  history = model.fit(X_train_f,y_train2,batch_size=20, epochs=1000,validation_split=0.1,verbose=0)\n",
        "  \n",
        "  # save the model\n",
        "  path=path_model\n",
        "  name_file=Name1+'_N_'+str(N_layer1)+'_L'\n",
        "  name_file1=os.path.join(path, name_file)\n",
        "  plot_model(model, show_shapes=True, to_file=name_file1+'_model.png')\n",
        "  model.save(name_file1+'_model.h5')\n",
        "  # summarize\n",
        "  print(model.summary())\n",
        "  time1 = time.clock() - start_time\n",
        "  Evaluate_model(model,history,name_file1,Name1,time1,score_table,X_test_f,y_test_f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3U84koIU7Pe"
      },
      "source": [
        "# Results Summary- Single DL model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOprzPwjbQzk"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "try:\n",
        "  os.mkdir('Single DL model')\n",
        "  #os.mkdir('classification2')\n",
        "except:\n",
        "  shutil.rmtree('Single DL model')\n",
        "  #shutil.rmtree('classification2')\n",
        "  os.mkdir('Single DL model')\n",
        "  #os.mkdir('classification2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q98l0bmfEkwp"
      },
      "source": [
        "window1=1\n",
        "N_layer1=1\n",
        "path_model='./Single DL model/'\n",
        "for n in [200,300]:\n",
        "  Name1=\"LSTM_\"+str(n)\n",
        "  fit_model(window1,n,N_layer1,X_train2,y_train,X_test2,y_test,Name1,path_model,score_table_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLUB_a2whfOf"
      },
      "source": [
        "score_table_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlaTkLnDVHot"
      },
      "source": [
        "# Results Summary- Single DL + additional layer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62U9Ous8YW6f"
      },
      "source": [
        "# initialize a score table to log the performance of various algorithms\n",
        "index = ['LSTM_'+str(n)+'| Added Layer' for n in [20,30,50,80]]\n",
        "score_table_3 = pd.DataFrame(index = index, \n",
        "                             columns= ['Time','Accuracy','precision','recall','F1','F_b','Fb_metric'])\n",
        "\n",
        "try:\n",
        "  os.mkdir('Single additional layer model')\n",
        "except:\n",
        "  shutil.rmtree('Single additional layer model')\n",
        "  os.mkdir('Single additional layer model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbe73lnRXgwT"
      },
      "source": [
        "window1=1\n",
        "N_layer1=2\n",
        "path_model='./Single additional layer model/'\n",
        "for n in [20,30,50,80]:\n",
        "  Name1=\"LSTM_\"+str(n)+'| Added Layer'\n",
        "  fit_model(window1,n,N_layer1,X_train2,y_train,X_test2,y_test,Name1,path_model,score_table_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUX-KlYFVIxd"
      },
      "source": [
        "score_table_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKytNvh8dzw3"
      },
      "source": [
        "## HPO Algorithm 7: Genetic Algorithm\n",
        "Genetic algorithms detect well-performing hyper-parameter combinations in each generation, and pass them to the next generation until the best-performing combination is identified.\n",
        "\n",
        "**Advantages:**\n",
        "* Efficient with all types of HPs.\n",
        "* Not require good initialization.\n",
        " \n",
        "\n",
        "**Disadvantages:**  \n",
        "* Poor capacity for parallelization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXhlPrMzdzw4"
      },
      "source": [
        "### Using DEAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_QpMfs0_-GQ"
      },
      "source": [
        "!pip install sklearn-deap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcAL6Zc3vNv0"
      },
      "source": [
        "col=df.columns\n",
        "X, y=df[col[1: ]], df[col[0]]\n",
        "X = preprocessing.minmax_scale(X)\n",
        "# split into train and test\n",
        "lab_enc = preprocessing.LabelEncoder()\n",
        "y = lab_enc.fit_transform(y)\n",
        "\n",
        "# split into train/test sets\n",
        "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "print(trainX.shape, testX.shape, trainy.shape ,testy.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSsQ1-u4dzw4"
      },
      "source": [
        "#Random Forest\n",
        "from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
        "# Define the hyperparameter configuration space\n",
        "rf_params = {\n",
        "    'n_estimators': np.logspace(1,1.8,num = 10 ,base=20,dtype='int'),\n",
        "    'max_depth': np.logspace(1,2,num = 10 ,base=10,dtype='int'),\n",
        "    \"max_features\":np.logspace(0.2,1,num = 5 ,base=8,dtype='int'),\n",
        "    \"min_samples_split\":np.logspace(0.4, 1, num=5, base=10, dtype='int'), #[2, 3, 5, 7, 10],\n",
        "    \"min_samples_leaf\":np.logspace(0.1,1,num = 5 ,base=11,dtype='int'),\n",
        "    \"criterion\":['gini','entropy']\n",
        "}\n",
        "rf_params = {\n",
        "    'n_estimators': range(10,100),\n",
        "    \"max_features\":range(1,trainX.shape[1]),\n",
        "    'max_depth': range(5,50),\n",
        "    \"min_samples_split\":range(2,11),\n",
        "    \"min_samples_leaf\":range(1,11),\n",
        "    #Categorical(name='criterion', categories=['gini','entropy'])#\n",
        "    \"criterion\":['gini','entropy']\n",
        "}\n",
        "clf = RandomForestClassifier(random_state=0)\n",
        "# Set the hyperparameters of GA \n",
        "ga1 = EvolutionaryAlgorithmSearchCV(estimator=clf,\n",
        "                                   params=rf_params,\n",
        "                                   scoring=\"accuracy\",\n",
        "                                   cv=3,\n",
        "                                   verbose=1,\n",
        "                                   population_size=10,\n",
        "                                   gene_mutation_prob=0.10,\n",
        "                                   gene_crossover_prob=0.5,\n",
        "                                   tournament_size=3,\n",
        "                                   generations_number=5,\n",
        "                                   n_jobs=1)\n",
        "ga1.fit(X,y)\n",
        "print(ga1.best_params_)\n",
        "print(\"Accuracy:\"+ str(ga1.best_score_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcLcVY9qTtQO"
      },
      "source": [
        "#SVM\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "rf_params = {\n",
        "    'C': np.random.uniform(0,50,1000),\n",
        "    \"kernel\":['linear','poly','rbf','sigmoid']\n",
        "}\n",
        "clf = SVC(gamma='scale')\n",
        "ga1 = EvolutionaryAlgorithmSearchCV(estimator=clf,\n",
        "                                   params=rf_params,\n",
        "                                   scoring=\"accuracy\",\n",
        "                                   cv=3,\n",
        "                                   verbose=1,\n",
        "                                   population_size=10,\n",
        "                                   gene_mutation_prob=0.10,\n",
        "                                   gene_crossover_prob=0.5,\n",
        "                                   tournament_size=3,\n",
        "                                   generations_number=5,\n",
        "                                   n_jobs=1)\n",
        "print(y)\n",
        "ga1.fit(X, y)\n",
        "print(ga1.best_params_)\n",
        "print(\"Accuracy:\"+ str(ga1.best_score_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_eU82flTyXC"
      },
      "source": [
        "#KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
        "rf_params = {\n",
        "    'n_neighbors': range(1,20),\n",
        "}\n",
        "clf = KNeighborsClassifier()\n",
        "ga1 = EvolutionaryAlgorithmSearchCV(estimator=clf,\n",
        "                                   params=rf_params,\n",
        "                                   scoring=\"accuracy\",\n",
        "                                   cv=3,\n",
        "                                   verbose=1,\n",
        "                                   population_size=10,\n",
        "                                   gene_mutation_prob=0.10,\n",
        "                                   gene_crossover_prob=0.5,\n",
        "                                   tournament_size=3,\n",
        "                                   generations_number=5,\n",
        "                                   n_jobs=1)\n",
        "ga1.fit(X, y)\n",
        "print(ga1.best_params_)\n",
        "print(\"Accuracy:\"+ str(ga1.best_score_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOpAPUgHAUN3"
      },
      "source": [
        "#ANN\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Input\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.callbacks import EarlyStopping\n",
        "def ANN(optimizer = 'sgd',neurons=32,batch_size=2,epochs=20,activation='relu',patience=3,loss='categorical_crossentropy'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(neurons, input_shape=(X.shape[1],), activation=activation))\n",
        "    model.add(Dense(neurons, activation=activation))\n",
        "    model.add(Dense(neurons, activation=activation))\n",
        "    model.add(Dense(neurons, activation=activation))\n",
        "    model.add(Dense(neurons, activation=activation))\n",
        "    model.add(Dense(2,activation='sigmoid'))  # 10 is the number of classes in the dataset, you can change it based on your dataset\n",
        "    model.compile(optimizer = optimizer, loss=loss)\n",
        "    early_stopping = EarlyStopping(monitor=\"loss\", patience = patience)# early stop patience\n",
        "    history = model.fit(X, pd.get_dummies(y).values,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              callbacks = [early_stopping],\n",
        "              verbose=0) #verbose set to 1 will show the training process\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELE0gMfCdzw6"
      },
      "source": [
        "#ANN\n",
        "from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
        "# Define the hyperparameter configuration space\n",
        "rf_params = {\n",
        "    'optimizer': ['adam','rmsprop','sgd'],\n",
        "    'activation': ['relu','tanh'],\n",
        "    'batch_size': [1, 2, 4, 8, 10],\n",
        "    'neurons':range(10,100),\n",
        "    'epochs':[20,50],\n",
        "    #'epochs':[20,50,100,200],\n",
        "    'patience':range(3,20)\n",
        "}\n",
        "clf = KerasClassifier(build_fn=ANN, verbose=0)\n",
        "# Set the hyperparameters of GA    \n",
        "ga1 = EvolutionaryAlgorithmSearchCV(estimator=clf,\n",
        "                                   params=rf_params,\n",
        "                                   scoring=\"accuracy\",\n",
        "                                   cv=3,\n",
        "                                   verbose=1,\n",
        "                                   population_size=10,\n",
        "                                   gene_mutation_prob=0.10,\n",
        "                                   gene_crossover_prob=0.5,\n",
        "                                   tournament_size=3,\n",
        "                                   generations_number=50,\n",
        "                                   n_jobs=1)\n",
        "ga1.fit(X, y)\n",
        "print(ga1.best_params_)\n",
        "print(\"Accuracy:\"+ str(ga1.best_score_))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}